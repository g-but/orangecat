# Timeline Posting Architecture Analysis

**Date**: 2025-11-13
**Purpose**: Analyze current implementation, compare with industry standards, recommend optimal path

---

## 1. Current OrangeCat Implementation

### Data Model

```sql
timeline_events (
  id uuid PRIMARY KEY,

  -- WHO did it
  actor_id uuid,          -- Person/org performing action
  actor_type text,        -- 'user' | 'organization' | 'system'

  -- WHAT happened
  event_type text,        -- 'status_update' | 'project_created' | etc.
  title text,
  description text,

  -- WHERE it appears (KEY CONCEPT)
  subject_type text,      -- 'profile' | 'project' | 'organization'
  subject_id uuid,        -- Whose timeline this appears on

  -- WHO/WHAT is mentioned (optional)
  target_type text,
  target_id uuid,

  -- Metadata
  visibility text,        -- 'public' | 'followers' | 'private'
  event_timestamp timestamptz,
  metadata jsonb
)
```

### Feed Generation Strategy: **Fan-Out on Read**

```typescript
// When user views their feed:
getCommunityFeed() {
  // Query: Scan timeline_events table
  SELECT * FROM timeline_events
  WHERE visibility = 'public'
  ORDER BY event_timestamp DESC
  LIMIT 20
}

// Enrichment happens at READ time:
enrichEventsForDisplay(events) {
  for each event:
    - Fetch actor profile (N+1 query)
    - Fetch subject details (N+1 query)
    - Fetch target details (N+1 query)
    - Fetch likes/comments counts (N+1 query)
}
```

### Current Performance Characteristics

- **Write**: Fast (single INSERT)
- **Read**: Slow (N+1 queries for enrichment)
- **Scalability**: Poor (full table scans for every feed load)

---

## 2. How Twitter Actually Does It

### Twitter's Evolution

#### Early Twitter (2006-2010): **Fan-Out on Read**

```
User posts tweet:
1. INSERT into tweets table
2. Done (instant)

User views timeline:
1. Get list of people user follows (100+ users)
2. Query recent tweets from those 100 users
3. Merge and sort by time
4. Return top 50
PROBLEM: Extremely slow for users following many people
```

#### Modern Twitter: **Hybrid Fan-Out**

**For Most Users (< 5000 followers): Fan-Out on Write**

```
User posts tweet:
1. INSERT into tweets table
2. Push to Redis timeline cache of EACH follower
   - If user has 1000 followers = 1000 cache writes
   - Async job, doesn't block user
3. Return immediately

User views timeline:
1. Read from personal Redis cache (instant)
2. Already sorted, already enriched
3. Occasionally refresh from database
```

**For Celebrities (> 5000 followers): Fan-Out on Read**

```
Celebrity posts tweet:
1. INSERT into tweets table only
2. Don't fan out (would be millions of writes)

User views timeline:
1. Hybrid approach:
   - Read cached timeline from most follows
   - Merge in recent tweets from celebrities
   - Sort and return
```

### Twitter's Data Architecture

```
â”œâ”€ tweets (PostgreSQL)          # Source of truth
â”œâ”€ timeline_cache (Redis)       # Per-user timelines
â”œâ”€ social_graph (Redis)         # Who follows whom
â”œâ”€ counts_cache (Redis)         # Likes, retweets, etc.
â””â”€ search_index (Elasticsearch) # Search and discovery
```

---

## 3. What OrangeCat Actually Needs

### Key Differences from Twitter

#### 1. **Scale**

- **Twitter**: 500M users, 6000 tweets/sec
- **OrangeCat**: ~1000-10000 users (realistic first year)

#### 2. **Use Case**

- **Twitter**: Real-time social media, high volume, ephemeral
- **OrangeCat**:
  - Reputation tracking (persistent, searchable)
  - Bitcoin transactions (append-only ledger)
  - Project milestones (historical record)
  - Community transparency (audit trail)

#### 3. **Post Frequency**

- **Twitter**: Users post 10-50x/day
- **OrangeCat**: Users post 1-5x/day (updates, not constant chatter)

#### 4. **Read Patterns**

- **Twitter**: 90% on "For You" feed (algorithmic)
- **OrangeCat**:
  - 40% on project pages (timeline of project history)
  - 30% on user profiles (reputation check)
  - 30% on community feed (discovery)

#### 5. **Data Retention**

- **Twitter**: Old tweets become less relevant
- **OrangeCat**: ALL history matters (reputation, transparency)

---

## 4. Current Problems in Our Implementation

### Problem 1: N+1 Query Hell

```typescript
// Current code:
async enrichEventsForDisplay(events: Event[]) {
  return Promise.all(events.map(async event => {
    const actor = await getActorInfo(event.actorId);      // Query 1 per event
    const subject = await getSubjectInfo(event.subjectId); // Query 2 per event
    const target = await getTargetInfo(event.targetId);    // Query 3 per event
    const likes = await getLikes(event.id);                // Query 4 per event
    // ...
  }));
}

// For 20 events = 80+ database queries
```

### Problem 2: No Caching

Every page load queries database from scratch.

### Problem 3: Missing Feed Pre-Generation

User's personal feed is computed on-demand, every time.

### Problem 4: Inefficient Indexes

```sql
-- Current indexes are good:
CREATE INDEX idx_timeline_actor_time ON timeline_events(actor_id, event_timestamp);
CREATE INDEX idx_timeline_subject_time ON timeline_events(subject_id, event_timestamp);

-- But queries still slow because of enrichment N+1
```

---

## 5. Recommended Architecture for OrangeCat

### Strategy: **Hybrid with Smart Caching**

Given:

- Small scale (1-10k users)
- Low post frequency
- High importance of historical data
- Limited resources (Supabase, no Redis infrastructure)
- Need for auditability and transparency

### Recommended Approach:

#### Phase 1: Optimize Current System (Immediate - 0 complexity)

**A. Eliminate N+1 Queries with JOINs**

```sql
-- Instead of separate queries, use JOINs:
CREATE VIEW enriched_timeline_events AS
SELECT
  te.*,
  -- Actor info
  actor.email as actor_email,
  actor.username as actor_username,
  actor.avatar_url as actor_avatar,
  -- Subject info
  CASE
    WHEN te.subject_type = 'profile' THEN profile.username
    WHEN te.subject_type = 'project' THEN project.title
  END as subject_name,
  -- Counts
  COUNT(DISTINCT likes.id) as like_count,
  COUNT(DISTINCT comments.id) as comment_count
FROM timeline_events te
LEFT JOIN profiles actor ON te.actor_id = actor.id
LEFT JOIN profiles profile ON te.subject_type = 'profile' AND te.subject_id = profile.id
LEFT JOIN projects project ON te.subject_type = 'project' AND te.subject_id = project.id
LEFT JOIN timeline_likes likes ON likes.event_id = te.id
LEFT JOIN timeline_comments comments ON comments.event_id = te.id
WHERE NOT te.is_deleted
GROUP BY te.id, actor.id, profile.id, project.id;

-- Now: 1 query instead of 80
```

**B. Add Materialized View for Community Feed**

```sql
-- Pre-compute public feed, refresh every 5 minutes
CREATE MATERIALIZED VIEW community_feed_cache AS
SELECT * FROM enriched_timeline_events
WHERE visibility = 'public'
ORDER BY event_timestamp DESC
LIMIT 1000;

CREATE UNIQUE INDEX ON community_feed_cache (id);

-- Refresh trigger (background job)
REFRESH MATERIALIZED VIEW CONCURRENTLY community_feed_cache;
```

**Performance Gain**: 20-50x faster feed loads with zero complexity

#### Phase 2: Add Lightweight Caching (Week 2 - Low complexity)

**Use Supabase Edge Functions + Vercel Edge Cache**

```typescript
// pages/api/timeline/community.ts
export const config = {
  runtime: 'edge',
};

export default async function handler(req: Request) {
  const cacheKey = `community-feed-${page}`;

  // Check Vercel KV cache (included free)
  const cached = await kv.get(cacheKey);
  if (cached) return cached;

  // Query materialized view
  const feed = await supabase
    .from('community_feed_cache')
    .select('*')
    .range(offset, offset + limit);

  // Cache for 2 minutes
  await kv.set(cacheKey, feed, { ex: 120 });

  return feed;
}
```

**Performance Gain**: Near-instant repeated loads, costs $0

#### Phase 3: Smart Feed Pre-Generation (Month 2 - Medium complexity)

**Only for Active Users**

```sql
-- Table for pre-generated personal feeds
CREATE TABLE user_feed_cache (
  user_id uuid,
  event_id uuid,
  position integer,  -- Sort order
  generated_at timestamptz,
  PRIMARY KEY (user_id, position)
);

-- Regenerate on cron job (every 5 min) for active users only
-- Active = logged in within last 24 hours
```

**When User Posts:**

```typescript
async function createPost(post) {
  // 1. Write to timeline_events
  const event = await supabase.from('timeline_events').insert(post);

  // 2. Invalidate relevant caches
  await invalidateCache(`user-feed-${post.actorId}`);
  await invalidateCache(`profile-${post.subjectId}`);
  await invalidateCache('community-feed');

  // 3. Queue feed regeneration for followers (async)
  //    Only if user has < 100 followers
  if (followerCount < 100) {
    await queue.add('regenerate-feeds', { userId: post.actorId });
  }
}
```

---

## 6. Comparison Table

| Aspect              | Twitter (2024)     | Current OrangeCat     | Recommended OrangeCat       |
| ------------------- | ------------------ | --------------------- | --------------------------- |
| **Write Strategy**  | Hybrid fan-out     | Direct write          | Direct write + invalidation |
| **Read Strategy**   | Cached timelines   | DB query + N+1        | Materialized view + cache   |
| **Data Storage**    | PostgreSQL + Redis | PostgreSQL (Supabase) | PostgreSQL + Vercel KV      |
| **Enrichment**      | Pre-computed       | Runtime (N+1)         | Pre-computed (JOIN)         |
| **Cache Layer**     | Redis (ms latency) | None                  | Vercel KV (ms latency)      |
| **Feed Generation** | Async workers      | On-demand             | Hybrid (MV + on-demand)     |
| **Complexity**      | Very High          | Low                   | Medium                      |
| **Cost**            | $$$$               | $                     | $$                          |
| **Scale**           | 500M users         | 1K users              | 10K+ users                  |
| **Query Time**      | 5-50ms             | 500-2000ms            | 20-100ms                    |

---

## 7. Implementation Priority

### Immediate (This Week)

âœ… **Already Done:**

- Timeline events table with proper indexes
- RLS policies for open posting
- Basic feed queries

ðŸ”§ **Next (High Impact, Low Effort):**

1. Create `enriched_timeline_events` VIEW with JOINs
2. Replace N+1 enrichment with single VIEW query
3. Add basic Vercel KV caching for community feed

**Estimated Impact**: 20x faster, 0 infrastructure changes

### Short Term (Month 1)

1. Materialized view for community feed
2. Background job to refresh MV every 5 min
3. Cache user profile timelines
4. Add timeline event search (Supabase full-text)

**Estimated Impact**: 50x faster, handles 10K users easily

### Long Term (Month 2-3)

1. Pre-generate feeds for active users
2. Implement smart cache invalidation
3. Add trending/algorithmic sorting
4. Real-time updates via Supabase Realtime

---

## 8. Key Architectural Decisions

### âœ… What We Got Right:

1. **Flexible Data Model**
   - Actor/Subject/Target pattern supports all use cases
   - Can post on profiles, projects, organizations
   - Tracks reputation and transparency

2. **Audit Trail**
   - All posts are immutable (soft delete only)
   - Full history preserved
   - Metadata tracks cross-posting

3. **Simple RLS**
   - Actor must be authenticated (no impersonation)
   - Public/followers/private visibility
   - Community can hold each other accountable

### âš ï¸ What Needs Improvement:

1. **Query Performance** (Critical)
   - N+1 queries must be eliminated
   - Need JOINs or denormalized data

2. **Caching Strategy** (High Priority)
   - Static feeds should be cached
   - Use Vercel Edge Cache (free)

3. **Feed Generation** (Medium Priority)
   - Materialized views for common queries
   - Background refresh for active feeds

---

## 9. Why This Is Better Than Twitter's Approach for Us

### Twitter's Approach Would Be **Overkill**:

1. **Redis Infrastructure**: Costs $50-500/month, adds complexity
2. **Fan-out on Write**: Unnecessary for low posting frequency
3. **Worker Queues**: Complex to maintain, not needed at our scale
4. **Real-time Everything**: Users don't need sub-second updates

### Our Approach Is **Right-Sized**:

1. **Use Existing Tools**: Supabase (already have) + Vercel KV (free)
2. **Optimize Queries**: JOINs and views (zero cost)
3. **Smart Caching**: Only cache hot paths
4. **Keep It Simple**: Can scale to 10K+ users before complexity needed

---

## 10. Code Changes Needed (Priority Order)

### 1. Create Enriched View (Highest Impact)

```sql
-- File: supabase/migrations/20251113150000_create_enriched_timeline_view.sql
CREATE OR REPLACE VIEW enriched_timeline_events AS
SELECT
  te.id,
  te.event_type,
  te.actor_id,
  te.subject_type,
  te.subject_id,
  te.title,
  te.description,
  te.event_timestamp,
  te.visibility,
  te.metadata,

  -- Actor data
  jsonb_build_object(
    'id', actor.id,
    'username', actor.username,
    'email', actor.email,
    'avatar_url', actor.avatar_url
  ) as actor_data,

  -- Subject data (polymorphic)
  CASE te.subject_type
    WHEN 'profile' THEN jsonb_build_object(
      'id', profile.id,
      'username', profile.username,
      'avatar_url', profile.avatar_url
    )
    WHEN 'project' THEN jsonb_build_object(
      'id', project.id,
      'title', project.title,
      'slug', project.slug
    )
  END as subject_data,

  -- Aggregated counts
  COALESCE(likes.count, 0) as like_count,
  COALESCE(comments.count, 0) as comment_count

FROM timeline_events te
LEFT JOIN profiles actor ON te.actor_id = actor.id
LEFT JOIN profiles profile ON te.subject_type = 'profile' AND te.subject_id = profile.id
LEFT JOIN projects project ON te.subject_type = 'project' AND te.subject_id = project.id
LEFT JOIN LATERAL (
  SELECT COUNT(*) as count
  FROM timeline_likes
  WHERE event_id = te.id
) likes ON true
LEFT JOIN LATERAL (
  SELECT COUNT(*) as count
  FROM timeline_comments
  WHERE event_id = te.id AND NOT is_deleted
) comments ON true
WHERE NOT te.is_deleted;
```

### 2. Update Service to Use View

```typescript
// src/services/timeline/index.ts

async getCommunityFeed() {
  // OLD (slow):
  // const events = await supabase.from('timeline_events').select('*');
  // const enriched = await this.enrichEventsForDisplay(events);

  // NEW (fast):
  const { data: events } = await supabase
    .from('enriched_timeline_events')  // â† Use view
    .select('*')
    .eq('visibility', 'public')
    .order('event_timestamp', { ascending: false })
    .range(offset, offset + limit - 1);

  // No enrichment needed - data already joined!
  return events.map(this.mapEnrichedEventToDisplay);
}
```

### 3. Add Basic Cache

```typescript
// src/lib/cache.ts
import { kv } from '@vercel/kv';

export async function getCachedFeed(key: string, fetcher: () => Promise<any>) {
  const cached = await kv.get(key);
  if (cached) return cached;

  const fresh = await fetcher();
  await kv.set(key, fresh, { ex: 120 }); // 2 min TTL
  return fresh;
}

// Usage:
const feed = await getCachedFeed('community-feed-1', () => getCommunityFeed({ page: 1 }));
```

---

## 11. Expected Performance After Optimizations

### Before (Current):

```
Community Feed Load Time: 1500-2500ms
- Query timeline_events: 50ms
- N+1 enrichment (20 events Ã— 4 queries): 1400ms
- Frontend rendering: 50ms

User Profile Load Time: 2000-3000ms
- Similar N+1 problem
```

### After Phase 1 (JOINs):

```
Community Feed Load Time: 100-200ms
- Query enriched view: 80ms
- No enrichment needed: 0ms
- Frontend rendering: 50ms

User Profile Load Time: 150-250ms
- Similar improvement
```

### After Phase 2 (Cache):

```
Community Feed Load Time (cached): 20-50ms
- Read from Vercel KV: 5ms
- Return cached data: 15ms

Community Feed Load Time (uncached): 120ms
- Query + cache write
```

---

## 12. Summary & Recommendations

### âœ… Do This Now (Week 1):

1. Create `enriched_timeline_events` VIEW
2. Update service layer to use VIEW
3. Add Vercel KV caching for community feed
4. Deploy and measure performance

**Cost**: 0 hours of work, $0 in infrastructure
**Gain**: 20x faster, supports 10K users

### ðŸ”„ Do This Soon (Month 1):

1. Materialized view + background refresh
2. Cache invalidation strategy
3. Profile timeline caching

**Cost**: 8-16 hours of work, $0 in infrastructure
**Gain**: 50x faster, supports 50K users

### ðŸš€ Do This Later (Month 2+):

1. Pre-generated feeds for power users
2. Algorithmic sorting/recommendations
3. Real-time updates via Supabase Realtime
4. Search with full-text indexing

**Cost**: 40+ hours of work, $20-50/month infrastructure
**Gain**: Twitter-like experience, supports 100K+ users

### ðŸš« Don't Do This (Ever, Unless $$$):

1. Redis infrastructure (unnecessary complexity)
2. Fan-out on write (wrong for our use case)
3. Microservices (overkill for our scale)
4. Kafka/message queues (way overkill)

---

## Conclusion

**Our current architecture is fundamentally sound** - the data model is flexible and the RLS policies enable the reputation system you envisioned.

**The main issue is query performance**, which can be fixed with simple optimizations (JOINs, views, caching) rather than complex infrastructure changes.

**Twitter's architecture is designed for 500M users posting thousands of times per second.** We're building for 1-10K users posting a few times per day. Our needs are completely different.

**Recommended path**: Fix the N+1 queries with VIEWs (this week), add caching (next week), then evaluate if anything else is needed based on actual usage patterns.

This gives us Twitter-like UX with 1/100th the complexity and cost.
