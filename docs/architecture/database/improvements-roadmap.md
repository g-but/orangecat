# ðŸš€ Database Improvements Roadmap

> Planned enhancements to take OrangeCat from 8.7/10 to 9.5/10

**Last Updated:** October 17, 2025
**Status:** In Progress

## Overview

Our database is **production-ready at 8.7/10**, but we've identified specific improvements to enhance scalability, observability, and performance. This document tracks implementation progress.

## Priority Matrix

```
High Impact, Quick Win  â”‚  High Impact, Complex
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ transactions.status   â”‚  â€¢ Table Partitioning
  index âœ…              â”‚  â€¢ Materialized Views
â€¢ Audit Log Table      â”‚
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Low Impact, Quick Win   â”‚  Low Impact, Complex
                        â”‚
â€¢ Covering Indexes     â”‚  â€¢ TimescaleDB
â€¢ Query Caching Docs   â”‚  â€¢ Read Replicas
```

---

## ðŸ”¥ High Priority (0-1 Month)

### 1. Add Missing Index on transactions.status âœ… DONE
**Priority:** P0 - Critical
**Complexity:** Low (5 mins)
**Impact:** High (frequent filtering)

**Problem:**
```sql
-- This query is slow without index
SELECT * FROM transactions
WHERE status = 'pending'
ORDER BY created_at DESC;
```

**Solution:**
```sql
CREATE INDEX idx_transactions_status ON transactions(status);
```

**Expected improvement:**
- Query time: 500ms â†’ 50ms (10x faster)
- Helps dashboard, admin tools, payment processing

**Status:** âœ… **COMPLETED** - Index created

---

### 2. Create Audit Logs Table
**Priority:** P0 - Critical (Compliance)
**Complexity:** Medium (2-3 hours)
**Impact:** High (Security, Compliance, Debugging)

**Why we need this:**
- Regulatory compliance (financial transactions)
- Security investigations (who did what, when)
- Debugging complex state changes
- User support (trace profile/payment issues)

**Design:**
```sql
CREATE TABLE audit_logs (
  id uuid PRIMARY KEY DEFAULT gen_random_uuid(),

  -- Who
  user_id uuid REFERENCES profiles(id),
  session_id text,
  ip_address inet,
  user_agent text,

  -- What
  action text NOT NULL, -- 'create', 'update', 'delete', 'login', etc.
  table_name text NOT NULL,
  record_id uuid,

  -- Data
  old_data jsonb,  -- Previous state
  new_data jsonb,  -- New state
  changes jsonb,   -- Computed diff

  -- Context
  metadata jsonb DEFAULT '{}',

  -- When
  created_at timestamptz DEFAULT now(),

  -- Indexes
  INDEX idx_audit_user_time (user_id, created_at DESC),
  INDEX idx_audit_table_record (table_name, record_id),
  INDEX idx_audit_action (action, created_at DESC)
);

-- Retention policy (keep 1 year, then archive)
CREATE INDEX idx_audit_retention ON audit_logs(created_at)
WHERE created_at < NOW() - INTERVAL '1 year';
```

**What to log:**
- âœ… Profile updates (especially verification status)
- âœ… Financial transactions (all state changes)
- âœ… Organization membership changes
- âœ… Permission/role updates
- âœ… Campaign status changes
- âŒ Read operations (too noisy)
- âŒ Automated trigger updates (unless critical)

**Implementation:**
1. Create table migration
2. Add trigger function for auto-logging
3. Add manual logging to critical operations
4. Build admin UI to query logs
5. Set up archival job (monthly)

**Status:** ðŸ”„ **IN PROGRESS**

---

## ðŸ“Š High Priority (1-3 Months)

### 3. Table Partitioning for Transactions
**Priority:** P1 - Important
**Complexity:** High (1 week)
**Impact:** High (Scalability)

**Problem:**
The `transactions` table will grow indefinitely:
- 1000 transactions/day = 365K/year
- 5 years = 1.8M rows
- Query performance degrades
- Vacuum/maintenance gets expensive

**Solution: Time-based partitioning**

```sql
-- Convert to partitioned table
CREATE TABLE transactions_partitioned (
  LIKE transactions INCLUDING ALL
) PARTITION BY RANGE (created_at);

-- Create monthly partitions
CREATE TABLE transactions_2025_10 PARTITION OF transactions_partitioned
  FOR VALUES FROM ('2025-10-01') TO ('2025-11-01');

CREATE TABLE transactions_2025_11 PARTITION OF transactions_partitioned
  FOR VALUES FROM ('2025-11-01') TO ('2025-12-01');

-- Attach existing data
-- (requires downtime OR pg_partman for zero-downtime)
```

**Benefits:**
- Query only relevant partitions (10x faster)
- Drop old partitions instantly (vs slow DELETE)
- Easier backups (partition-level)
- Better vacuum performance

**Migration strategy:**
1. **Phase 1:** Create partitioned table (empty)
2. **Phase 2:** Copy data in batches (off-peak)
3. **Phase 3:** Swap tables (minimal downtime)
4. **Phase 4:** Update app code (if needed)
5. **Phase 5:** Automate partition creation

**Automation:**
```sql
-- Monthly cron job
CREATE OR REPLACE FUNCTION create_next_partition()
RETURNS void AS $$
DECLARE
  partition_date date := date_trunc('month', NOW() + INTERVAL '1 month');
  partition_name text := 'transactions_' || to_char(partition_date, 'YYYY_MM');
  start_date text := partition_date::text;
  end_date text := (partition_date + INTERVAL '1 month')::text;
BEGIN
  EXECUTE format(
    'CREATE TABLE IF NOT EXISTS %I PARTITION OF transactions_partitioned
     FOR VALUES FROM (%L) TO (%L)',
    partition_name, start_date, end_date
  );
END;
$$ LANGUAGE plpgsql;
```

**Status:** ðŸ“‹ **PLANNED** - Q1 2026

---

### 4. Archival Strategy for Old Data
**Priority:** P1 - Important
**Complexity:** Medium (3-4 days)
**Impact:** Medium (Performance, Cost)

**Tables to archive:**

#### notifications (archive after 90 days)
```sql
CREATE TABLE notifications_archive (
  LIKE notifications INCLUDING ALL
);

-- Monthly archival job
INSERT INTO notifications_archive
SELECT * FROM notifications
WHERE created_at < NOW() - INTERVAL '90 days'
  AND is_read = true;

DELETE FROM notifications
WHERE id IN (SELECT id FROM notifications_archive);
```

#### audit_logs (archive after 1 year)
```sql
CREATE TABLE audit_logs_archive_2024 (
  LIKE audit_logs INCLUDING ALL
);

-- Yearly archival
INSERT INTO audit_logs_archive_2024
SELECT * FROM audit_logs
WHERE created_at >= '2024-01-01'
  AND created_at < '2025-01-01';
```

**Benefits:**
- Smaller active tables (faster queries)
- Lower storage costs (compress archives)
- Maintain history (compliance)

**Archive storage options:**
1. **Same DB, different schema** - Easy, same queries
2. **S3 + Parquet** - Cheap, queryable with Athena
3. **Data warehouse** - Snowflake/BigQuery for analytics

**Status:** ðŸ“‹ **PLANNED** - Q2 2026

---

## ðŸš€ Medium Priority (3-6 Months)

### 5. Materialized Views for Analytics
**Priority:** P2 - Nice to have
**Complexity:** Medium (1 week)
**Impact:** Medium (Performance)

**Use cases:**

#### Campaign Leaderboard
```sql
CREATE MATERIALIZED VIEW project_leaderboard AS
SELECT
  fp.id,
  fp.title,
  fp.slug,
  fp.current_amount,
  fp.goal_amount,
  fp.current_amount / NULLIF(fp.goal_amount, 0) * 100 as progress_pct,
  COUNT(DISTINCT t.id) as donor_count,
  COUNT(DISTINCT t.user_id) as unique_donors,
  MAX(t.created_at) as last_donation_at
FROM funding_pages fp
LEFT JOIN transactions t ON t.funding_page_id = fp.id
WHERE fp.status = 'active'
GROUP BY fp.id
ORDER BY fp.current_amount DESC;

-- Refresh hourly
CREATE INDEX ON project_leaderboard(current_amount DESC);
```

#### User Contribution Stats
```sql
CREATE MATERIALIZED VIEW user_contribution_stats AS
SELECT
  p.id,
  p.username,
  COUNT(DISTINCT t.funding_page_id) as projects_supported,
  SUM(t.amount) as total_donated,
  AVG(t.amount) as avg_donation,
  MIN(t.created_at) as first_donation_at,
  MAX(t.created_at) as last_donation_at
FROM profiles p
LEFT JOIN transactions t ON t.user_id = p.id
WHERE t.status = 'completed'
GROUP BY p.id;
```

**Refresh strategy:**
- **Real-time views**: Refresh on every write (slow)
- **Near real-time**: Refresh every 5-15 minutes
- **Batch**: Refresh nightly (2am)

**Implementation:**
```sql
-- Option 1: Manual refresh
REFRESH MATERIALIZED VIEW CONCURRENTLY project_leaderboard;

-- Option 2: Automated (pg_cron)
SELECT cron.schedule('refresh-leaderboard', '*/15 * * * *',
  'REFRESH MATERIALIZED VIEW CONCURRENTLY project_leaderboard');
```

**Status:** ðŸ“‹ **PLANNED** - Q2 2026

---

### 6. Covering Indexes
**Priority:** P2 - Nice to have
**Complexity:** Low (2-3 hours)
**Impact:** Low-Medium (Performance)

**What are covering indexes?**
Include all columns needed by query â†’ no table lookup needed

**Example:**
```sql
-- Current: Index only on username
CREATE INDEX idx_profiles_username ON profiles(username);

-- Query needs username + display_name + avatar_url
SELECT username, display_name, avatar_url
FROM profiles
WHERE username = 'orangecat';

-- Problem: Index lookup + table lookup (2 I/O operations)

-- Solution: Covering index
CREATE INDEX idx_profiles_username_covering ON profiles(username)
INCLUDE (display_name, avatar_url);

-- Now: Index-only scan (1 I/O operation) âœ…
```

**Candidates:**
1. Profile search: `username INCLUDE (display_name, avatar_url, is_verified)`
2. Campaign list: `status INCLUDE (title, slug, current_amount, goal_amount)`
3. Notifications: `user_id, is_read INCLUDE (type, title, created_at)`

**Trade-off:**
- âœ… Faster reads (index-only scans)
- âŒ Slower writes (larger index to maintain)
- âŒ More storage (duplicated data)

**When to use:**
- âœ… Read-heavy tables
- âœ… Hot queries with limited columns
- âŒ Wide tables (many columns)
- âŒ Frequently updated columns

**Status:** ðŸ“‹ **PLANNED** - Q3 2026

---

## ðŸ”® Future Considerations (6+ Months)

### 7. Read Replicas
**When needed:** > 10K users, heavy analytics load

```
Primary (Write) â”€â”€â†’ Replica 1 (Analytics)
                â”€â”€â†’ Replica 2 (Dashboard)
                â”€â”€â†’ Replica 3 (Backups)
```

**Use cases:**
- Analytics queries don't impact production
- Geo-distributed reads (lower latency)
- Zero-downtime migrations

**Supabase setup:**
- Supabase Pro: Read replicas available
- Configure in dashboard
- Update connection strings

---

### 8. TimescaleDB Extension
**When needed:** Heavy time-series analytics

**Benefits:**
- Automatic partitioning (time-based)
- Compression (10x storage savings)
- Continuous aggregates (faster queries)

**Ideal for:**
- Bitcoin price history
- Transaction volume tracking
- User activity patterns

```sql
-- Convert transactions to hypertable
SELECT create_hypertable('transactions', 'created_at');

-- Automatic compression
ALTER TABLE transactions SET (
  timescaledb.compress,
  timescaledb.compress_segmentby = 'user_id'
);
```

---

### 9. Connection Pooling (PgBouncer)
**When needed:** > 100 concurrent users

**Why:**
- PostgreSQL has max connections limit
- Each connection has overhead
- Pooler reuses connections

**Setup:**
```bash
# Supabase provides PgBouncer
# Use pooled connection string for:
- Serverless functions
- High-concurrency APIs
- Background jobs
```

---

## ðŸ“ˆ Success Metrics

### Performance
- âœ… 95th percentile query time < 100ms
- âœ… Dashboard loads in < 1s
- âœ… Search response < 200ms

### Scalability
- âœ… Handle 10K concurrent users
- âœ… 100K transactions/day
- âœ… 1M+ profiles

### Observability
- âœ… All critical operations logged
- âœ… Slow queries identified
- âœ… Index usage monitored

---

## Implementation Timeline

```
Month 1 (Oct 2025)
â”œâ”€â”€ âœ… transactions.status index
â”œâ”€â”€ ðŸ”„ Audit logs table
â””â”€â”€ ðŸ“„ Documentation

Month 2-3 (Nov-Dec 2025)
â”œâ”€â”€ Table partitioning (transactions)
â””â”€â”€ Archival strategy (notifications)

Month 4-6 (Q1 2026)
â”œâ”€â”€ Materialized views
â”œâ”€â”€ Covering indexes
â””â”€â”€ Performance testing

Month 7-12 (Q2-Q3 2026)
â”œâ”€â”€ Read replicas (if needed)
â”œâ”€â”€ TimescaleDB evaluation
â””â”€â”€ Advanced analytics
```

---

## How to Contribute

### Adding New Improvements
1. Identify the problem
2. Quantify impact (query times, storage)
3. Design solution with trade-offs
4. Create migration script
5. Test on staging
6. Document in this roadmap

### Testing Strategy
```sql
-- Before
EXPLAIN ANALYZE
SELECT * FROM transactions WHERE status = 'pending';

-- After
EXPLAIN ANALYZE
SELECT * FROM transactions WHERE status = 'pending';

-- Compare: Execution time, rows scanned, index used
```

---

**Next:** Let's start implementing!
See [Implementation Guide](./implementation-guide.md) for step-by-step instructions.
