# AI Model System - Complete Guide

**Last Updated:** 2026-01-18
**Status:** Implemented

---

## Overview

OrangeCat's AI model system is designed to be **effortless for beginners** while **powerful for advanced users**.

**The golden rule:** Users can start chatting with AI immediately, without any setup or configuration.

---

## User Experience

### For Non-Technical Users (Default)

```
1. Sign up to OrangeCat
2. Open My Cat chat
3. Start talking immediately
   â†“
âœ“ Free AI model automatically selected
âœ“ Zero configuration needed
âœ“ 100% private (no data stored)
âœ“ 10 free messages per day
```

**Model Used:** Llama 4 Maverick (free tier via OpenRouter)
- Fast responses
- Good quality
- No API costs
- Rate limited to prevent abuse

### For Power Users (Optional Upgrade)

Users can optionally:

1. **Add API Key** (BYOK - Bring Your Own Key)
   - OpenRouter key â†’ Access to 200+ models
   - OR individual keys (OpenAI, Anthropic, Google, etc.)

2. **Run Locally** (Ultimate Privacy)
   - Install Ollama
   - Download models
   - 100% private, zero cloud data

3. **Choose Specific Models**
   - GPT-4o for best quality
   - Claude 3.5 for reasoning
   - Gemini 2.0 for huge context
   - Grok for real-time data

---

## Technical Architecture

### Model Registry (SSOT)

**Location:** `src/config/model-registry.ts`

Single source of truth for all AI models:

```typescript
export const MODEL_REGISTRY: Record<string, ModelMetadata> = {
  'groq/mixtral-8x7b': {
    id: 'groq/mixtral-8x7b',
    name: 'Mixtral 8x7B',
    provider: 'Groq',
    tier: 'freemium',
    availability: 'cloud',
    requiresApiKey: false, // Use server key
    // ... more metadata
  },

  'openai/gpt-4o': {
    id: 'openai/gpt-4o',
    name: 'GPT-4o',
    provider: 'OpenAI',
    tier: 'paid',
    requiresApiKey: true, // User must provide
    // ... more metadata
  },

  'local/llama-3.1-8b': {
    id: 'local/llama-3.1-8b',
    name: 'Llama 3.1 8B (Local)',
    provider: 'Meta (via Ollama)',
    tier: 'free',
    availability: 'local',
    requiresApiKey: false, // Runs on user's computer
    // ... more metadata
  },
};
```

**Benefits:**
- âœ… One place to define all models
- âœ… Easy to add new models
- âœ… Consistent metadata across app
- âœ… Type-safe model selection

---

### Unified AI Client

**Location:** `src/lib/ai/unified-client.ts`

Single interface for all providers:

```typescript
const client = new UnifiedAIClient(config);

// Works with ANY model
const response = await client.chat({
  model: 'groq/mixtral-8x7b', // OR 'openai/gpt-4o' OR 'local/llama-3.1-8b'
  messages: [{ role: 'user', content: 'Hello!' }],
  stream: true,
});
```

**Supported Providers:**
- âœ… OpenRouter (200+ models, one API key)
- âœ… OpenAI (GPT-4o, GPT-4o-mini, etc.)
- âœ… Anthropic (Claude 3.5 Sonnet, Opus 4)
- âœ… Google (Gemini 2.0 Flash, Pro)
- âœ… X.AI (Grok 2)
- âœ… Groq (ultra-fast inference)
- âœ… Together AI (open models)
- âœ… Ollama (local models)
- âœ… LM Studio (local models)

---

### API Endpoint

**Location:** `src/app/api/cat/chat/route.ts`

**Default Behavior:**
```typescript
// Non-technical user sends message
POST /api/cat/chat
{
  "message": "I want to make money"
}

// Response uses FREE model automatically
// No configuration needed
// User doesn't even know which model was used (unless they check)
```

**Power User with BYOK:**
```typescript
// User with API key can choose model
POST /api/cat/chat
Headers: { 'x-openrouter-key': 'sk-...' }
{
  "message": "I want to make money",
  "model": "anthropic/claude-3.5-sonnet" // Specific model
}

// OR auto-select best model
{
  "message": "Complex reasoning task",
  "model": "auto" // Auto-router selects best model
}
```

---

## Model Selection Logic

### Priority System

```
1. User Specifies Model â†’ Use that model
2. User Has BYOK â†’ Auto-select from all models
3. User on Free Tier â†’ Auto-select from free models only
4. Fallback â†’ DEFAULT_FREE_MODEL_ID
```

### Auto-Router

Smart model selection based on task:

```typescript
const route = autoRouter.selectModel({
  message: "Write a complex function",
  conversationHistory: [...],
  allowedModels: availableModels,
});

// Returns best model for the task
// - Code tasks â†’ Models with function calling
// - Creative tasks â†’ Models with large context
// - Vision tasks â†’ Models with vision support
```

---

## Security

### API Key Storage

**Three Tiers:**

#### 1. Free Tier (Zero User Risk)
```typescript
// Server uses its own API key
const apiKey = process.env.GROQ_API_KEY; // Server-side only

// User never sees this
// User never pays for this
// It's OrangeCat's cost
```

#### 2. Ephemeral BYOK (Most Secure)
```typescript
// User's key sent per-request in header
// NEVER stored in database
// Cleared from memory after use

fetch('/api/cat/chat', {
  headers: {
    'x-openrouter-key': userKey, // Ephemeral
  },
});
```

#### 3. Encrypted Storage (Opt-In Convenience)
```typescript
// User explicitly opts in
// Password-based encryption
// Zero-knowledge (server can't decrypt)

const encrypted = await encryptApiKey(apiKey, userPassword);
// Store encrypted blob in DB
// Decrypt on client with password
```

**Default:** Ephemeral (most secure)

---

## Model Categories

### Free Tier
- âœ… Llama 4 Maverick (default)
- âœ… Llama 3.3 70B
- âœ… Gemini 2.0 Flash
- âœ… DeepSeek R1
- âœ… Qwen QWQ 32B

**Limits:** 10 messages/day on platform key

### Premium (BYOK Required)
- ğŸ’ GPT-4o (best overall)
- ğŸ’ Claude 3.5 Sonnet (best reasoning)
- ğŸ’ Claude Opus 4 (most capable)
- ğŸ’ Gemini 2.0 Pro (2M context)
- ğŸ’ Grok 2 (real-time data)

**Limits:** User's API key limits

### Local (Ultimate Privacy)
- ğŸ”’ Llama 3.1 8B (fast, 16GB RAM)
- ğŸ”’ Llama 3.1 70B (best quality, 64GB RAM)
- ğŸ”’ Mistral 7B (lightweight, 8GB RAM)

**Limits:** Hardware only

---

## User Interface

### Model Status Badge

**Location:** `src/components/ai-chat/ModelStatusBadge.tsx`

Shows current model with hover details:

```tsx
<ModelStatusBadge
  modelName="Llama 4 Maverick"
  isFree={true}
  messagesRemaining={7}
  onUpgrade={() => showModelSelector()}
/>
```

**Appearance:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ âš¡ Free     â”‚  â† Hover for details
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**On Hover:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ âœ¨ Current Model                 â”‚
â”‚ Llama 4 Maverick                 â”‚
â”‚                                  â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚ â”‚ Free Messages  7 remaining â”‚  â”‚
â”‚ â”‚                            â”‚  â”‚
â”‚ â”‚ You're using a free AI     â”‚  â”‚
â”‚ â”‚ model. No setup required.  â”‚  â”‚
â”‚ â”‚                            â”‚  â”‚
â”‚ â”‚ [Upgrade to Premium]       â”‚  â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                  â”‚
â”‚ âœ“ Benefits:                      â”‚
â”‚ â€¢ No data stored                 â”‚
â”‚ â€¢ 100% private                   â”‚
â”‚ â€¢ Fast responses                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**For BYOK Users:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ‘‘ GPT-4o   â”‚  â† Crown icon
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Implementation Status

### âœ… Completed

- [x] Model Registry created
- [x] UnifiedAIClient built
- [x] Cat chat API updated
- [x] Default free tier working
- [x] BYOK support implemented
- [x] Model status badge created
- [x] Security measures in place

### ğŸš§ Next Steps

- [ ] Model selector UI component
- [ ] Local model detection & setup wizard
- [ ] OpenRouter integration testing
- [ ] Add model switching to chat UI
- [ ] Model comparison view
- [ ] Usage tracking per model
- [ ] Cost calculator for paid models

---

## Adding New Models

### 1. Add to Registry

```typescript
// src/config/model-registry.ts

'newprovider/newmodel': {
  id: 'newprovider/newmodel',
  name: 'New Model Name',
  provider: 'New Provider',
  tier: 'free', // or 'paid'
  availability: 'cloud', // or 'local' or 'both'
  requiresApiKey: true, // or false
  // ... other metadata
},
```

### 2. Add Provider Handler (if needed)

```typescript
// src/lib/ai/unified-client.ts

private async chatNewProvider(options, modelMeta) {
  const apiKey = this.config.apiKeys?.newprovider;

  return fetch('https://api.newprovider.com/chat', {
    method: 'POST',
    headers: {
      'Authorization': `Bearer ${apiKey}`,
    },
    body: JSON.stringify({
      model: options.model,
      messages: options.messages,
    }),
  });
}
```

### 3. Route in Main Client

```typescript
// src/lib/ai/unified-client.ts

case 'New Provider':
  return this.chatNewProvider(options, modelMeta);
```

### 4. Done!

Model is now available everywhere:
- âœ… Cat chat
- âœ… AI assistants
- âœ… Auto-router
- âœ… Model selector UI

---

## Best Practices

### For Developers

**DO:**
- âœ… Always use MODEL_REGISTRY for model metadata
- âœ… Use UnifiedAIClient for all AI calls
- âœ… Handle streaming responses properly
- âœ… Show loading states
- âœ… Handle errors gracefully
- âœ… Track usage for free tier users

**DON'T:**
- âŒ Hardcode model IDs in components
- âŒ Bypass the unified client
- âŒ Store API keys in plaintext
- âŒ Expose API keys in client code
- âŒ Make assumptions about model availability

### For UX

**DO:**
- âœ… Default to free tier (zero friction)
- âœ… Show which model is being used (badge)
- âœ… Explain upgrade benefits clearly
- âœ… Make model switching easy
- âœ… Show usage limits transparently

**DON'T:**
- âŒ Force users to configure before use
- âŒ Hide which model is being used
- âŒ Surprise users with costs
- âŒ Make local setup too technical
- âŒ Overwhelm with model choices

---

## References

- **Model Registry:** `src/config/model-registry.ts`
- **Unified Client:** `src/lib/ai/unified-client.ts`
- **Chat API:** `src/app/api/cat/chat/route.ts`
- **Status Badge:** `src/components/ai-chat/ModelStatusBadge.tsx`
- **Existing Models:** `src/config/ai-models.ts`

---

## Support

For questions or issues:
- Check model registry for available models
- Verify API keys are correctly configured
- Check free tier usage limits
- Test with different models
- Review error messages in console

---

**Remember:** The goal is **instant AI access** for everyone, with optional power features for those who want them.
